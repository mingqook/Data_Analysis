{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "alive-tissue",
   "metadata": {},
   "source": [
    "# Python_Machine Learning_Pandas_Data Analysis - part 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-stack",
   "metadata": {},
   "source": [
    "##### 데이터 분석 관련 내용을 정리 하였으며, 파이썬 머신러닝 판다스 데이터분석(오승환 지음)을 참고하여 작성하였습니다.\n",
    "##### 해당 자료는 python 3.7 기반으로 작성되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-report",
   "metadata": {},
   "source": [
    "## 5 데이터 사전 처리\n",
    "### 5.1 누락 데이터 처리\n",
    "#### - 데이터 분석의 정확도는 분석 데이터의 품질에 좌우된다. 데이터 품질 향상을 위해 누락 데이터, 중복 데이터 등 오류를 수정하고 분석 목적에 맞게 변형하는 과정이 필요하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "elementary-government",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df = sns.load_dataset('titanic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "constitutional-macro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
      "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
      "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
      "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
      "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
      "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
      "\n",
      "     who  adult_male deck  embark_town alive  alone  \n",
      "0    man        True  NaN  Southampton    no  False  \n",
      "1  woman       False    C    Cherbourg   yes  False  \n",
      "2  woman       False  NaN  Southampton   yes   True  \n",
      "3  woman       False    C  Southampton   yes  False  \n",
      "4    man        True  NaN  Southampton    no   True  \n",
      "----------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 15 columns):\n",
      " #   Column       Non-Null Count  Dtype   \n",
      "---  ------       --------------  -----   \n",
      " 0   survived     891 non-null    int64   \n",
      " 1   pclass       891 non-null    int64   \n",
      " 2   sex          891 non-null    object  \n",
      " 3   age          714 non-null    float64 \n",
      " 4   sibsp        891 non-null    int64   \n",
      " 5   parch        891 non-null    int64   \n",
      " 6   fare         891 non-null    float64 \n",
      " 7   embarked     889 non-null    object  \n",
      " 8   class        891 non-null    category\n",
      " 9   who          891 non-null    object  \n",
      " 10  adult_male   891 non-null    bool    \n",
      " 11  deck         203 non-null    category\n",
      " 12  embark_town  889 non-null    object  \n",
      " 13  alive        891 non-null    object  \n",
      " 14  alone        891 non-null    bool    \n",
      "dtypes: bool(2), category(2), float64(2), int64(4), object(5)\n",
      "memory usage: 80.7+ KB\n",
      "None\n",
      "----------------------------------\n",
      "NaN    688\n",
      "C       59\n",
      "B       47\n",
      "D       33\n",
      "E       32\n",
      "A       15\n",
      "F       13\n",
      "G        4\n",
      "Name: deck, dtype: int64\n",
      "----------------------------------\n",
      "   survived  pclass    sex    age  sibsp  parch   fare  embarked  class  \\\n",
      "0     False   False  False  False  False  False  False     False  False   \n",
      "1     False   False  False  False  False  False  False     False  False   \n",
      "2     False   False  False  False  False  False  False     False  False   \n",
      "3     False   False  False  False  False  False  False     False  False   \n",
      "4     False   False  False  False  False  False  False     False  False   \n",
      "\n",
      "     who  adult_male   deck  embark_town  alive  alone  \n",
      "0  False       False   True        False  False  False  \n",
      "1  False       False  False        False  False  False  \n",
      "2  False       False   True        False  False  False  \n",
      "3  False       False  False        False  False  False  \n",
      "4  False       False   True        False  False  False  \n",
      "----------------------------------\n",
      "   survived  pclass   sex   age  sibsp  parch  fare  embarked  class   who  \\\n",
      "0      True    True  True  True   True   True  True      True   True  True   \n",
      "1      True    True  True  True   True   True  True      True   True  True   \n",
      "2      True    True  True  True   True   True  True      True   True  True   \n",
      "3      True    True  True  True   True   True  True      True   True  True   \n",
      "4      True    True  True  True   True   True  True      True   True  True   \n",
      "\n",
      "   adult_male   deck  embark_town  alive  alone  \n",
      "0        True  False         True   True   True  \n",
      "1        True   True         True   True   True  \n",
      "2        True  False         True   True   True  \n",
      "3        True   True         True   True   True  \n",
      "4        True  False         True   True   True  \n",
      "----------------------------------\n",
      "survived       0\n",
      "pclass         0\n",
      "sex            0\n",
      "age            0\n",
      "sibsp          0\n",
      "parch          0\n",
      "fare           0\n",
      "embarked       0\n",
      "class          0\n",
      "who            0\n",
      "adult_male     0\n",
      "deck           3\n",
      "embark_town    0\n",
      "alive          0\n",
      "alone          0\n",
      "dtype: int64\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# head() - df의 첫 5행 확인\n",
    "print(df.head())\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "# info() - df의 요약 정보 확인 / null이 아닌 개수를 출력\n",
    "print(df.info())\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "# value_counts() - dropna = False를 통해 누락 데이터 개수 확인 가능\n",
    "nan_deck = df['deck'].value_counts(dropna = False)\n",
    "print(nan_deck)\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "# isnull() - 누락 데이터면 True / notnull() - 누락 데이터면 False\n",
    "print(df.head().isnull())\n",
    "print(\"----------------------------------\")\n",
    "print(df.head().notnull())\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "# isnull() & sum(axis = 0) - isnull() 결과 값의 행을 기준으로 합하면 각 열의 누락 데이터 합을 구할 수 있다.\n",
    "print(df.head().isnull().sum(axis = 0))\n",
    "print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "becoming-twelve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "survived :  0\n",
      "pclass :  0\n",
      "sex :  0\n",
      "age :  177\n",
      "sibsp :  0\n",
      "parch :  0\n",
      "fare :  0\n",
      "embarked :  2\n",
      "class :  0\n",
      "who :  0\n",
      "adult_male :  0\n",
      "deck :  688\n",
      "embark_town :  2\n",
      "alive :  0\n",
      "alone :  0\n",
      "----------------------------------\n",
      "Index(['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare',\n",
      "       'embarked', 'class', 'who', 'adult_male', 'embark_town', 'alive',\n",
      "       'alone'],\n",
      "      dtype='object')\n",
      "----------------------------------\n",
      "714\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# for문을 활용하여 각 열의 nan 개수 계산\n",
    "missing_df = df.isnull()\n",
    "for col in missing_df.columns:\n",
    "    missing_count = missing_df[col].value_counts()\n",
    "    \n",
    "    try:\n",
    "        print(col, \": \", missing_count[True])\n",
    "    except:\n",
    "        print(col, \": \", 0)\n",
    "print(\"----------------------------------\")\n",
    "        \n",
    "# dropna() - thresh 옵션을 적용하여 특정 기준 이상의 nan 개수를 갖는 열을 삭제한다. / axis = 1 - 열 삭제\n",
    "df_thread = df.dropna(axis = 1, thresh = 500)\n",
    "print(df_thread.columns)\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "# age 열에 나이 데이터가 없는 모든 행 삭제 / axis = 0 - 행 삭제 / how = 'any' - 하나라도 존재하면 삭제 / how = 'all' - 모든 데이터가 nan이면 삭제\n",
    "df_age = df.dropna(subset = ['age'], how = 'any', axis = 0)\n",
    "print(len(df_age))\n",
    "print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "transparent-affiliation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan 값 채우기 전 : 0      22.0\n",
      "1      38.0\n",
      "2      26.0\n",
      "3      35.0\n",
      "4      35.0\n",
      "       ... \n",
      "886    27.0\n",
      "887    19.0\n",
      "888     NaN\n",
      "889    26.0\n",
      "890    32.0\n",
      "Name: age, Length: 891, dtype: float64\n",
      "----------------------------------\n",
      "nan 값 채운 후 : 0      22.000000\n",
      "1      38.000000\n",
      "2      26.000000\n",
      "3      35.000000\n",
      "4      35.000000\n",
      "         ...    \n",
      "886    27.000000\n",
      "887    19.000000\n",
      "888    29.699118\n",
      "889    26.000000\n",
      "890    32.000000\n",
      "Name: age, Length: 891, dtype: float64\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# fillna() - 누락된 값을 특정 값으로 대체 / inplace = True 를 통해 원본 데이터 변경 가능\n",
    "print('nan 값 채우기 전 :', df['age']) \n",
    "print(\"----------------------------------\")\n",
    "\n",
    "mean_age = df['age'].mean(axis = 0) # age의 평균\n",
    "df['age'].fillna(mean_age, inplace = True)\n",
    "\n",
    "print('nan 값 채운 후 :', df['age'])\n",
    "print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-delhi",
   "metadata": {},
   "source": [
    "#### - 데이터 중 null 값이 ?, * 와 같은 의미없는 값으로 입력되는 경우가 많다. pandas에서 null 데이터를 다루기 위해서는 다음과 같은 값을 replace() 메소드를 활용하여 numpy에서 지원하는 np.nan으로 변경하는 것이 좋다.\n",
    "#### - 예시 : ?을 np.nan으로 치환 -> df.replace('?', np.nan, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "advance-matthew",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "누락된 값 채우기 전:  825     Queenstown\n",
      "826    Southampton\n",
      "827      Cherbourg\n",
      "828     Queenstown\n",
      "829            NaN\n",
      "Name: embark_town, dtype: object\n",
      "----------------------------------\n",
      "누락된 값 채운 후:  825     Queenstown\n",
      "826    Southampton\n",
      "827      Cherbourg\n",
      "828     Queenstown\n",
      "829     Queenstown\n",
      "Name: embark_town, dtype: object\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# fillna() - method = 'ffill' : 직전 행의 값으로 채워줌 / method = 'bfill' : 직후 행의 값으로 채워줌\n",
    "print('누락된 값 채우기 전: ', df['embark_town'][825:830])\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "df['embark_town'].fillna(method = 'ffill', inplace = True)\n",
    "print('누락된 값 채운 후: ', df['embark_town'][825:830])\n",
    "print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capable-reverse",
   "metadata": {},
   "source": [
    "### 5.2 중복 데이터 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "sustainable-theme",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  c1  c2  c3\n",
      "0  a   1   1\n",
      "1  a   1   1\n",
      "2  b   1   2\n",
      "3  a   2   2\n",
      "4  b   2   2\n",
      "----------------------------------\n",
      "0    False\n",
      "1     True\n",
      "2    False\n",
      "3    False\n",
      "4    False\n",
      "dtype: bool\n",
      "----------------------------------\n",
      "0    False\n",
      "1     True\n",
      "2     True\n",
      "3    False\n",
      "4     True\n",
      "Name: c2, dtype: bool\n",
      "----------------------------------\n",
      "  c1  c2  c3\n",
      "0  a   1   1\n",
      "2  b   1   2\n",
      "3  a   2   2\n",
      "4  b   2   2\n",
      "----------------------------------\n",
      "  c1  c2  c3\n",
      "0  a   1   1\n",
      "2  b   1   2\n",
      "3  a   2   2\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# duplicated() - 행의 레코드가 중복되는 지 확인 - 처음 나오는 값은 False 반환\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'c1' : ['a', 'a', 'b', 'a', 'b'],\n",
    "                  'c2' : [1,1,1,2,2,],\n",
    "                  'c3' : [1,1,2,2,2]})\n",
    "\n",
    "print(df)\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "df_dup = df.duplicated()\n",
    "print(df_dup)\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "# seires에도 duplicated() 적용 가능\n",
    "col_dup = df['c2'].duplicated()\n",
    "print(col_dup)\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "# drop_duplicates() - 중복데이터 제거\n",
    "df2 = df.drop_duplicates()\n",
    "print(df2)\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "# drop_duplicates() - subset 옵션에 '열 이름 리스트' 전달 하여 해당 열들을 기준으로 중복을 판단\n",
    "df3 = df.drop_duplicates(subset = ['c2' ,'c3'])\n",
    "print(df3)\n",
    "print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-watson",
   "metadata": {},
   "source": [
    "### 5.3 데이터 표준화\n",
    "#### - 데이터 포맷을 일관성 있게 표준화하는 작업"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-pregnancy",
   "metadata": {},
   "source": [
    "#### 5.3.1 단위 환산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "supposed-member",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mpg  cylinders  displacement horsepower  weight  acceleration  model year  \\\n",
      "0  18.0          8         307.0      130.0  3504.0          12.0          70   \n",
      "1  15.0          8         350.0      165.0  3693.0          11.5          70   \n",
      "2  18.0          8         318.0      150.0  3436.0          11.0          70   \n",
      "3  16.0          8         304.0      150.0  3433.0          12.0          70   \n",
      "4  17.0          8         302.0      140.0  3449.0          10.5          70   \n",
      "\n",
      "   origin                       name  \n",
      "0       1  chevrolet chevelle malibu  \n",
      "1       1          buick skylark 320  \n",
      "2       1         plymouth satellite  \n",
      "3       1              amc rebel sst  \n",
      "4       1                ford torino  \n",
      "----------------------------------\n",
      "    mpg  cylinders  displacement horsepower  weight  acceleration  model year  \\\n",
      "0  18.0          8         307.0      130.0  3504.0          12.0          70   \n",
      "1  15.0          8         350.0      165.0  3693.0          11.5          70   \n",
      "2  18.0          8         318.0      150.0  3436.0          11.0          70   \n",
      "3  16.0          8         304.0      150.0  3433.0          12.0          70   \n",
      "4  17.0          8         302.0      140.0  3449.0          10.5          70   \n",
      "\n",
      "   origin                       name       kpl  \n",
      "0       1  chevrolet chevelle malibu  7.652571  \n",
      "1       1          buick skylark 320  6.377143  \n",
      "2       1         plymouth satellite  7.652571  \n",
      "3       1              amc rebel sst  6.802286  \n",
      "4       1                ford torino  7.227428  \n",
      "----------------------------------\n",
      "    mpg  cylinders  displacement horsepower  weight  acceleration  model year  \\\n",
      "0  18.0          8         307.0      130.0  3504.0          12.0          70   \n",
      "1  15.0          8         350.0      165.0  3693.0          11.5          70   \n",
      "2  18.0          8         318.0      150.0  3436.0          11.0          70   \n",
      "3  16.0          8         304.0      150.0  3433.0          12.0          70   \n",
      "4  17.0          8         302.0      140.0  3449.0          10.5          70   \n",
      "\n",
      "   origin                       name   kpl  \n",
      "0       1  chevrolet chevelle malibu  7.65  \n",
      "1       1          buick skylark 320  6.38  \n",
      "2       1         plymouth satellite  7.65  \n",
      "3       1              amc rebel sst  6.80  \n",
      "4       1                ford torino  7.23  \n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('auto-mpg.csv', header = None)\n",
    "\n",
    "df.columns = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model year', 'origin', 'name']\n",
    "print(df.head())\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "# mpg를 kpl로 변환(mpg to kpl = 0.425)\n",
    "mpg_to_kpl = 1.60934 / 3.78541\n",
    "\n",
    "df['kpl'] = df['mpg'] * mpg_to_kpl\n",
    "print(df.head(5))\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "df['kpl'] = df['kpl'].round(2)\n",
    "print(df.head(5))\n",
    "print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-confirmation",
   "metadata": {},
   "source": [
    "#### 5.3.2 자료형 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cordless-anger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpg              float64\n",
      "cylinders          int64\n",
      "displacement     float64\n",
      "horsepower       float64\n",
      "weight           float64\n",
      "acceleration     float64\n",
      "model year      category\n",
      "origin            object\n",
      "name              object\n",
      "kpl              float64\n",
      "dtype: object\n",
      "----------------------------------\n",
      "[130. 165. 150. 140. 198. 220. 215. 225. 190. 170. 160.  95.  97.  85.\n",
      "  88.  46.  87.  90. 113. 200. 210. 193. 100. 105. 175. 153. 180. 110.\n",
      "  72.  86.  70.  76.  65.  69.  60.  80.  54. 208. 155. 112.  92. 145.\n",
      " 137. 158. 167.  94. 107. 230.  49.  75.  91. 122.  67.  83.  78.  52.\n",
      "  61.  93. 148. 129.  96.  71.  98. 115.  53.  81.  79. 120. 152. 102.\n",
      " 108.  68.  58. 149.  89.  63.  48.  66. 139. 103. 125. 133. 138. 135.\n",
      " 142.  77.  62. 132.  84.  64.  74. 116.  82.]\n",
      "----------------------------------\n",
      "float64\n",
      "----------------------------------\n",
      "category\n",
      "----------------------------------\n",
      "object\n",
      "----------------------------------\n",
      "357    81\n",
      "378    82\n",
      "204    76\n",
      "Name: model year, dtype: category\n",
      "Categories (13, int64): [70, 71, 72, 73, ..., 79, 80, 81, 82]\n",
      "----------------------------------\n",
      "146    74\n",
      "53     71\n",
      "216    77\n",
      "Name: model year, dtype: category\n",
      "Categories (13, int64): [70, 71, 72, 73, ..., 79, 80, 81, 82]\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "# unique() - 고유값 확인\n",
    "print(df['horsepower'].unique())\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "# 누락데이터의 형태를 변경 후 삭제 한 뒤 type 변환\n",
    "import numpy as np\n",
    "df['horsepower'].replace('?', np.nan, inplace = True)\n",
    "df.dropna(subset = ['horsepower'], axis = 0, inplace = True)\n",
    "df['horsepower'] = df['horsepower'].astype('float')\n",
    "print(df['horsepower'].dtypes)\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "# str -> category\n",
    "df['origin'] = df['origin'].astype('category')\n",
    "print(df['origin'].dtypes)\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "# category -> str\n",
    "df['origin'] = df['origin'].astype('str')\n",
    "print(df['origin'].dtypes)\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "# sample() - random choice\n",
    "print(df['model year'].sample(3))\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "# int -> category\n",
    "df['model year'] = df['model year'].astype('category')\n",
    "print(df['model year'].sample(3))\n",
    "print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-cancellation",
   "metadata": {},
   "source": [
    "### 5.4 범주형 데이터 처리\n",
    "#### 5.4.1 구간 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "moderate-charlotte",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 46.         107.33333333 168.66666667 230.        ]\n",
      "----------------------------------\n",
      "    horsepower hp_bin\n",
      "0        130.0   보통출력\n",
      "1        165.0   보통출력\n",
      "2        150.0   보통출력\n",
      "3        150.0   보통출력\n",
      "4        140.0   보통출력\n",
      "5        198.0    고출력\n",
      "6        220.0    고출력\n",
      "7        215.0    고출력\n",
      "8        225.0    고출력\n",
      "9        190.0    고출력\n",
      "10       170.0    고출력\n",
      "11       160.0   보통출력\n",
      "12       150.0   보통출력\n",
      "13       225.0    고출력\n",
      "14        95.0    저출력\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('auto-mpg.csv', header = None)\n",
    "\n",
    "df.columns = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model year', 'origin', 'name']\n",
    "\n",
    "df['horsepower'].replace('?', np.nan, inplace = True)\n",
    "df.dropna(subset = ['horsepower'], axis = 0, inplace = True)\n",
    "df['horsepower'] = df['horsepower'].astype('float')\n",
    "\n",
    "# np.histogram() - 경계값을 구하는 방법 - bins 옵션에 입력하여 구간에 속하는 값의 개수와 경계값 리스트 반환\n",
    "count, bin_dividers = np.histogram(df['horsepower'], bins = 3)\n",
    "print(bin_dividers)\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "# pd.cut() - 경계값 할당 - include_lowest = True : 구간의 낮은 경계값을 포함\n",
    "bin_names = ['저출력', '보통출력', '고출력']\n",
    "df['hp_bin'] = pd.cut(x = df['horsepower'], \n",
    "                     bins = bin_dividers,\n",
    "                     labels=bin_names,\n",
    "                     include_lowest=True)\n",
    "print(df[['horsepower', 'hp_bin']].head(15))\n",
    "print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-tomorrow",
   "metadata": {},
   "source": [
    "#### 5.4.2 더미 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cleared-victory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    저출력  보통출력  고출력\n",
      "0     0     1    0\n",
      "1     0     1    0\n",
      "2     0     1    0\n",
      "3     0     1    0\n",
      "4     0     1    0\n",
      "5     0     0    1\n",
      "6     0     0    1\n",
      "7     0     0    1\n",
      "8     0     0    1\n",
      "9     0     0    1\n",
      "10    0     0    1\n",
      "11    0     1    0\n",
      "12    0     1    0\n",
      "13    0     0    1\n",
      "14    1     0    0\n",
      "----------------------------------\n",
      "[1 1 1 1 1 0 0 0 0 0 0 1 1 0 2]\n",
      "----------------------------------\n",
      "<class 'numpy.ndarray'>\n",
      "----------------------------------\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [2]]\n",
      "----------------------------------\n",
      "<class 'numpy.ndarray'>\n",
      "----------------------------------\n",
      "  (0, 1)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (2, 1)\t1.0\n",
      "  (3, 1)\t1.0\n",
      "  (4, 1)\t1.0\n",
      "  (5, 0)\t1.0\n",
      "  (6, 0)\t1.0\n",
      "  (7, 0)\t1.0\n",
      "  (8, 0)\t1.0\n",
      "  (9, 0)\t1.0\n",
      "  (10, 0)\t1.0\n",
      "  (11, 1)\t1.0\n",
      "  (12, 1)\t1.0\n",
      "  (13, 0)\t1.0\n",
      "  (14, 2)\t1.0\n",
      "----------------------------------\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# pd.get_dummies() - 범주형 변수의 모든 고유값을 각각 새로운 더비 면수로 변환\n",
    "horsepower_dummies = pd.get_dummies(df['hp_bin'])\n",
    "print(horsepower_dummies.head(15))\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "# sklearn 라이브러리를 활용한 one-hot encoding\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# 전처리를 위한 encoder 객체 만들기 \n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "onehot_encoder = preprocessing.OneHotEncoder()\n",
    "\n",
    "# label_encoder로 문자열 범주를 숫자형 범주로 변환\n",
    "onehot_labeled = label_encoder.fit_transform(df['hp_bin'].head(15))\n",
    "print(onehot_labeled)\n",
    "print(\"----------------------------------\")\n",
    "print(type(onehot_labeled))\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "# 2차원 행렬로 변경\n",
    "onehot_reshaped = onehot_labeled.reshape(len(onehot_labeled), 1)\n",
    "print(onehot_reshaped)\n",
    "print(\"----------------------------------\")\n",
    "print(type(onehot_reshaped))\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "# 희소행렬로 변환\n",
    "onehot_fitted = onehot_encoder.fit_transform(onehot_reshaped)\n",
    "print(onehot_fitted)\n",
    "print(\"----------------------------------\")\n",
    "print(type(onehot_fitted))\n",
    "print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-muslim",
   "metadata": {},
   "source": [
    "### 5.5 정규화\n",
    "#### - 각 열(변수)에 속하는 데이터 값을 동일한 크기 기준으로 나눈 비율을 나타내는 것을 정규화라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "disturbed-visibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    392.000000\n",
      "mean     104.469388\n",
      "std       38.491160\n",
      "min       46.000000\n",
      "25%       75.000000\n",
      "50%       93.500000\n",
      "75%      126.000000\n",
      "max      230.000000\n",
      "Name: horsepower, dtype: float64\n",
      "----------------------------------\n",
      "0    0.565217\n",
      "1    0.717391\n",
      "2    0.652174\n",
      "3    0.652174\n",
      "4    0.608696\n",
      "Name: horsepower, dtype: float64\n",
      "----------------------------------\n",
      "count    392.000000\n",
      "mean       0.454215\n",
      "std        0.167353\n",
      "min        0.200000\n",
      "25%        0.326087\n",
      "50%        0.406522\n",
      "75%        0.547826\n",
      "max        1.000000\n",
      "Name: horsepower, dtype: float64\n",
      "----------------------------------\n",
      "0    0.456522\n",
      "1    0.646739\n",
      "2    0.565217\n",
      "3    0.565217\n",
      "4    0.510870\n",
      "Name: horsepower, dtype: float64\n",
      "----------------------------------\n",
      "count    392.000000\n",
      "mean       0.317768\n",
      "std        0.209191\n",
      "min        0.000000\n",
      "25%        0.157609\n",
      "50%        0.258152\n",
      "75%        0.434783\n",
      "max        1.000000\n",
      "Name: horsepower, dtype: float64\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('auto-mpg.csv', header = None)\n",
    "\n",
    "df.columns = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model year', 'origin', 'name']\n",
    "\n",
    "df['horsepower'].replace('?', np.nan, inplace = True)\n",
    "df.dropna(subset = ['horsepower'], axis = 0, inplace = True)\n",
    "df['horsepower'] = df['horsepower'].astype('float')\n",
    "\n",
    "print(df['horsepower'].describe())\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "df['horsepower'] = df['horsepower'] / abs(df['horsepower'].max())\n",
    "print(df['horsepower'].head())\n",
    "print(\"----------------------------------\")\n",
    "print(df['horsepower'].describe())\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "min_x = df['horsepower'] - df['horsepower'].min()\n",
    "min_max = df['horsepower'].max() - df['horsepower'].min()\n",
    "df['horsepower'] = min_x / min_max\n",
    "print(df['horsepower'].head())\n",
    "print(\"----------------------------------\")\n",
    "print(df['horsepower'].describe())\n",
    "print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-motor",
   "metadata": {},
   "source": [
    "### 5.6 시계열 데이터\n",
    "#### 5.6.1 다른 자료형을 시계열 객체로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "capital-gossip",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  Close  Start   High    Low  Volume\n",
      "0  2018-07-02  10100  10850  10900  10000  137977\n",
      "1  2018-06-29  10700  10550  10900   9990  170253\n",
      "2  2018-06-28  10400  10900  10950  10150  155769\n",
      "3  2018-06-27  10900  10800  11050  10500  133548\n",
      "4  2018-06-26  10800  10900  11000  10700   63039\n",
      "----------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20 entries, 0 to 19\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Date    20 non-null     object\n",
      " 1   Close   20 non-null     int64 \n",
      " 2   Start   20 non-null     int64 \n",
      " 3   High    20 non-null     int64 \n",
      " 4   Low     20 non-null     int64 \n",
      " 5   Volume  20 non-null     int64 \n",
      "dtypes: int64(5), object(1)\n",
      "memory usage: 1.1+ KB\n",
      "None\n",
      "----------------------------------\n",
      "         Date  Close  Start   High    Low  Volume   new_Date\n",
      "0  2018-07-02  10100  10850  10900  10000  137977 2018-07-02\n",
      "1  2018-06-29  10700  10550  10900   9990  170253 2018-06-29\n",
      "2  2018-06-28  10400  10900  10950  10150  155769 2018-06-28\n",
      "3  2018-06-27  10900  10800  11050  10500  133548 2018-06-27\n",
      "4  2018-06-26  10800  10900  11000  10700   63039 2018-06-26\n",
      "----------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20 entries, 0 to 19\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype         \n",
      "---  ------    --------------  -----         \n",
      " 0   Date      20 non-null     object        \n",
      " 1   Close     20 non-null     int64         \n",
      " 2   Start     20 non-null     int64         \n",
      " 3   High      20 non-null     int64         \n",
      " 4   Low       20 non-null     int64         \n",
      " 5   Volume    20 non-null     int64         \n",
      " 6   new_Date  20 non-null     datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int64(5), object(1)\n",
      "memory usage: 1.2+ KB\n",
      "None\n",
      "----------------------------------\n",
      "<class 'pandas._libs.tslibs.timestamps.Timestamp'>\n",
      "----------------------------------\n",
      "            Close  Start   High    Low  Volume\n",
      "new_Date                                      \n",
      "2018-07-02  10100  10850  10900  10000  137977\n",
      "2018-06-29  10700  10550  10900   9990  170253\n",
      "2018-06-28  10400  10900  10950  10150  155769\n",
      "2018-06-27  10900  10800  11050  10500  133548\n",
      "2018-06-26  10800  10900  11000  10700   63039\n",
      "----------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 20 entries, 2018-07-02 to 2018-06-01\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   Close   20 non-null     int64\n",
      " 1   Start   20 non-null     int64\n",
      " 2   High    20 non-null     int64\n",
      " 3   Low     20 non-null     int64\n",
      " 4   Volume  20 non-null     int64\n",
      "dtypes: int64(5)\n",
      "memory usage: 960.0 bytes\n",
      "None\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('stock-data.csv')\n",
    "print(df.head())\n",
    "print(\"----------------------------------\")\n",
    "print(df.info())\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "# pd.to_datetime() - pandas의 timestamp(시점)을 나타내는 datetime64 타입으로 변환\n",
    "df['new_Date'] = pd.to_datetime(df['Date'])\n",
    "print(df.head())\n",
    "print(\"----------------------------------\")\n",
    "print(df.info())\n",
    "print(\"----------------------------------\")\n",
    "print(type(df['new_Date'][0]))\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "# 바꾼 datetime을 index 지정 시 datetimeindex로 저장\n",
    "df.set_index('new_Date', inplace = True)\n",
    "df.drop('Date', axis = 1, inplace = True)\n",
    "print(df.head())\n",
    "print(\"----------------------------------\")\n",
    "print(df.info())\n",
    "print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cardiovascular-pillow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2019-01-01', '2020-03-01', '2021-06-01'], dtype='datetime64[ns]', freq=None)\n",
      "----------------------------------\n",
      "PeriodIndex(['2019-01-01', '2020-03-01', '2021-06-01'], dtype='period[D]', freq='D')\n",
      "----------------------------------\n",
      "PeriodIndex(['2019-01', '2020-03', '2021-06'], dtype='period[M]', freq='M')\n",
      "----------------------------------\n",
      "PeriodIndex(['2019', '2020', '2021'], dtype='period[A-DEC]', freq='A-DEC')\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "dates = ['2019-01-01', '2020-03-01', '2021-06-01']\n",
    "\n",
    "ts_dates = pd.to_datetime(dates)\n",
    "print(ts_dates)\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "# to_period() - 일정한 기간을 나타내는 period 객체로 timestamp 객체를 변환 / freq 옵션에 기준이 되는 기간 설정\n",
    "pr_day = ts_dates.to_period(freq = 'D')\n",
    "print(pr_day)\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "pr_month = ts_dates.to_period(freq = 'M')\n",
    "print(pr_month)\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "pr_year = ts_dates.to_period(freq = 'Y')\n",
    "print(pr_year)\n",
    "print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-conditions",
   "metadata": {},
   "source": [
    "#### 5.6.2 시계열 데이터 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "arctic-major",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2019-01-01 00:00:00+09:00', '2019-02-01 00:00:00+09:00',\n",
      "               '2019-03-01 00:00:00+09:00', '2019-04-01 00:00:00+09:00',\n",
      "               '2019-05-01 00:00:00+09:00', '2019-06-01 00:00:00+09:00'],\n",
      "              dtype='datetime64[ns, Asia/Seoul]', freq='MS')\n",
      "----------------------------------\n",
      "DatetimeIndex(['2019-01-31 00:00:00+09:00', '2019-02-28 00:00:00+09:00',\n",
      "               '2019-03-31 00:00:00+09:00', '2019-04-30 00:00:00+09:00',\n",
      "               '2019-05-31 00:00:00+09:00', '2019-06-30 00:00:00+09:00'],\n",
      "              dtype='datetime64[ns, Asia/Seoul]', freq='M')\n",
      "----------------------------------\n",
      "DatetimeIndex(['2019-01-31 00:00:00+09:00', '2019-04-30 00:00:00+09:00',\n",
      "               '2019-07-31 00:00:00+09:00', '2019-10-31 00:00:00+09:00',\n",
      "               '2020-01-31 00:00:00+09:00', '2020-04-30 00:00:00+09:00'],\n",
      "              dtype='datetime64[ns, Asia/Seoul]', freq='3M')\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# pd.date_range() - 여러 개의 날짜가 들어 있는 배열 형태의 시계열 데이터 생성\n",
    "ts_ms = pd.date_range(start = '2019-01-01', # 시작점\n",
    "                     end = None,            # 끝점\n",
    "                     periods = 6,           # 생성할 timestamp 개수\n",
    "                     freq = 'MS',           # 시간 간격(MS : 월의 시작일)\n",
    "                     tz = 'Asia/Seoul')     # 시간대(timezone) - 한국\n",
    "\n",
    "print(ts_ms)\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "ts_me = pd.date_range(start = '2019-01-01', # 시작점\n",
    "                     end = None,            # 끝점\n",
    "                     periods = 6,           # 생성할 timestamp 개수\n",
    "                     freq = 'M',           # 시간 간격(M : 월의 마지막 날)\n",
    "                     tz = 'Asia/Seoul')     # 시간대(timezone) - 한국\n",
    "\n",
    "print(ts_me)\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "ts_3m = pd.date_range(start = '2019-01-01', # 시작점\n",
    "                     end = None,            # 끝점\n",
    "                     periods = 6,           # 생성할 timestamp 개수\n",
    "                     freq = '3M',           # 시간 간격(3M : 3개월)\n",
    "                     tz = 'Asia/Seoul')     # 시간대(timezone) - 한국\n",
    "\n",
    "print(ts_3m)\n",
    "print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "historical-walker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeriodIndex(['2019-01', '2019-02', '2019-03'], dtype='period[M]', freq='M')\n",
      "----------------------------------\n",
      "PeriodIndex(['2019-01-01 00:00', '2019-01-01 01:00', '2019-01-01 02:00'], dtype='period[H]', freq='H')\n",
      "----------------------------------\n",
      "PeriodIndex(['2019-01-01 00:00', '2019-01-01 02:00', '2019-01-01 04:00'], dtype='period[2H]', freq='2H')\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# pd.period_range() - 여러 개의 기간이 들어 있는 시계열 데이터 생성\n",
    "pr_m = pd.period_range(start = '2019-01-01',\n",
    "                      end = None,\n",
    "                      periods = 3,\n",
    "                      freq = 'M')     # 기간의 길이(M:월)\n",
    "print(pr_m)\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "pr_h = pd.period_range(start = '2019-01-01',\n",
    "                      end = None,\n",
    "                      periods = 3,\n",
    "                      freq = 'H')     # 기간의 길이(H: 1시간)\n",
    "print(pr_h)\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "pr_2h = pd.period_range(start = '2019-01-01',\n",
    "                      end = None,\n",
    "                      periods = 3,\n",
    "                      freq = '2H')     # 기간의 길이(2H: 2시간)\n",
    "print(pr_2h)\n",
    "print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-exercise",
   "metadata": {},
   "source": [
    "#### 5.6.3 시계열 데이터 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "domestic-prerequisite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  Close  Start   High    Low  Volume   new_Date\n",
      "0  2018-07-02  10100  10850  10900  10000  137977 2018-07-02\n",
      "1  2018-06-29  10700  10550  10900   9990  170253 2018-06-29\n",
      "2  2018-06-28  10400  10900  10950  10150  155769 2018-06-28\n",
      "3  2018-06-27  10900  10800  11050  10500  133548 2018-06-27\n",
      "4  2018-06-26  10800  10900  11000  10700   63039 2018-06-26\n",
      "----------------------------------\n",
      "         Date  Close  Start   High    Low  Volume   new_Date  Year  Month  Day\n",
      "0  2018-07-02  10100  10850  10900  10000  137977 2018-07-02  2018      7    2\n",
      "1  2018-06-29  10700  10550  10900   9990  170253 2018-06-29  2018      6   29\n",
      "2  2018-06-28  10400  10900  10950  10150  155769 2018-06-28  2018      6   28\n",
      "3  2018-06-27  10900  10800  11050  10500  133548 2018-06-27  2018      6   27\n",
      "4  2018-06-26  10800  10900  11000  10700   63039 2018-06-26  2018      6   26\n",
      "----------------------------------\n",
      "         Date  Close  Start   High    Low  Volume   new_Date  Year  Month  \\\n",
      "0  2018-07-02  10100  10850  10900  10000  137977 2018-07-02  2018      7   \n",
      "1  2018-06-29  10700  10550  10900   9990  170253 2018-06-29  2018      6   \n",
      "2  2018-06-28  10400  10900  10950  10150  155769 2018-06-28  2018      6   \n",
      "3  2018-06-27  10900  10800  11050  10500  133548 2018-06-27  2018      6   \n",
      "4  2018-06-26  10800  10900  11000  10700   63039 2018-06-26  2018      6   \n",
      "\n",
      "   Day Date_yr   Date_m  \n",
      "0    2    2018  2018-07  \n",
      "1   29    2018  2018-06  \n",
      "2   28    2018  2018-06  \n",
      "3   27    2018  2018-06  \n",
      "4   26    2018  2018-06  \n",
      "----------------------------------\n",
      "               Date  Close  Start   High    Low  Volume   new_Date  Year  \\\n",
      "Date_m                                                                     \n",
      "2018-07  2018-07-02  10100  10850  10900  10000  137977 2018-07-02  2018   \n",
      "2018-06  2018-06-29  10700  10550  10900   9990  170253 2018-06-29  2018   \n",
      "2018-06  2018-06-28  10400  10900  10950  10150  155769 2018-06-28  2018   \n",
      "2018-06  2018-06-27  10900  10800  11050  10500  133548 2018-06-27  2018   \n",
      "2018-06  2018-06-26  10800  10900  11000  10700   63039 2018-06-26  2018   \n",
      "\n",
      "         Month  Day Date_yr  \n",
      "Date_m                       \n",
      "2018-07      7    2    2018  \n",
      "2018-06      6   29    2018  \n",
      "2018-06      6   28    2018  \n",
      "2018-06      6   27    2018  \n",
      "2018-06      6   26    2018  \n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('stock-data.csv')\n",
    "df['new_Date'] = pd.to_datetime(df['Date'])\n",
    "print(df.head())\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "# dt 속성을 이용하여 열의 연-월-일 정보를 년, 월, 일로 구분\n",
    "df['Year'] = df['new_Date'].dt.year\n",
    "df['Month'] = df['new_Date'].dt.month\n",
    "df['Day'] = df['new_Date'].dt.day\n",
    "print(df.head())\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "# timestamp를 period로 변환하여 연-월-일 표기 변경\n",
    "df['Date_yr'] = df['new_Date'].dt.to_period(freq = 'A')\n",
    "df['Date_m'] = df['new_Date'].dt.to_period(freq = 'M')\n",
    "print(df.head())\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "df.set_index('Date_m', inplace = True)\n",
    "print(df.head())\n",
    "print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "american-fusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Date  Close  Start   High    Low  Volume\n",
      "new_Date                                                  \n",
      "2018-07-02  2018-07-02  10100  10850  10900  10000  137977\n",
      "2018-06-29  2018-06-29  10700  10550  10900   9990  170253\n",
      "2018-06-28  2018-06-28  10400  10900  10950  10150  155769\n",
      "2018-06-27  2018-06-27  10900  10800  11050  10500  133548\n",
      "2018-06-26  2018-06-26  10800  10900  11000  10700   63039\n",
      "----------------------------------\n",
      "DatetimeIndex(['2018-07-02', '2018-06-29', '2018-06-28', '2018-06-27',\n",
      "               '2018-06-26', '2018-06-25', '2018-06-22', '2018-06-21',\n",
      "               '2018-06-20', '2018-06-19', '2018-06-18', '2018-06-15',\n",
      "               '2018-06-14', '2018-06-12', '2018-06-11', '2018-06-08',\n",
      "               '2018-06-07', '2018-06-05', '2018-06-04', '2018-06-01'],\n",
      "              dtype='datetime64[ns]', name='new_Date', freq=None)\n",
      "----------------------------------\n",
      "                  Date  Close  Start   High    Low  Volume\n",
      "new_Date                                                  \n",
      "2018-07-02  2018-07-02  10100  10850  10900  10000  137977\n",
      "2018-06-29  2018-06-29  10700  10550  10900   9990  170253\n",
      "2018-06-28  2018-06-28  10400  10900  10950  10150  155769\n",
      "2018-06-27  2018-06-27  10900  10800  11050  10500  133548\n",
      "2018-06-26  2018-06-26  10800  10900  11000  10700   63039\n",
      "----------------------------------\n",
      "                  Date  Close  Start   High    Low  Volume\n",
      "new_Date                                                  \n",
      "2018-07-02  2018-07-02  10100  10850  10900  10000  137977\n",
      "----------------------------------\n",
      "            Start   High\n",
      "new_Date                \n",
      "2018-07-02  10850  10900\n",
      "----------------------------------\n",
      "                  Date  Close  Start   High    Low  Volume\n",
      "new_Date                                                  \n",
      "2018-07-02  2018-07-02  10100  10850  10900  10000  137977\n",
      "----------------------------------\n",
      "                  Date  Close  Start   High    Low  Volume\n",
      "new_Date                                                  \n",
      "2018-06-25  2018-06-25  11150  11400  11450  11000   55519\n",
      "2018-06-22  2018-06-22  11300  11250  11450  10750  134805\n",
      "2018-06-21  2018-06-21  11200  11350  11750  11200  133002\n",
      "2018-06-20  2018-06-20  11550  11200  11600  10900  308596\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qook/Python/3.7/venv/lib/python3.7/site-packages/ipykernel_launcher.py:9: FutureWarning: Indexing a DataFrame with a datetimelike index using a single string to slice the rows, like `frame[string]`, is deprecated and will be removed in a future version. Use `frame.loc[string]` instead.\n",
      "  if __name__ == '__main__':\n",
      "/Users/qook/Python/3.7/venv/lib/python3.7/site-packages/ipykernel_launcher.py:21: FutureWarning: Indexing a DataFrame with a datetimelike index using a single string to slice the rows, like `frame[string]`, is deprecated and will be removed in a future version. Use `frame.loc[string]` instead.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('stock-data.csv')\n",
    "df['new_Date'] = pd.to_datetime(df['Date'])\n",
    "df.set_index('new_Date', inplace = True)\n",
    "print(df.head())\n",
    "print(\"----------------------------------\")\n",
    "print(df.index)\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "df_y = df['2018']\n",
    "print(df_y.head())\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "df_ym = df.loc['2018-07']\n",
    "print(df_ym)\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "df_ym_cols = df.loc['2018-07', 'Start':'High']\n",
    "print(df_ym_cols)\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "df_ymd = df['2018-07-02']\n",
    "print(df_ymd)\n",
    "print(\"----------------------------------\")\n",
    "\n",
    "df_ymd_range = df['2018-06-25' : '2018-06-20']\n",
    "print(df_ymd_range)\n",
    "print(\"----------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
